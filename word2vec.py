#!/usr/bin/env python

import numpy as np
import sys
sys.path.insert(0, "./activations")
from dnn import *
from softmax import *

"""
TODO: check if you have to normalize columns instead of rows
TODO: If linear_activation_forward() take as input the hyper_parameters python array,
      you should update softmax_gradient()
TODO: update softmax_gradient() using functions in dnn.py
"""

def normalize_rows(X):
    """
    Row normalization function. Implement a function that normalizes each
    row of a matrix to have unit length

    Arguments:
    X -- data, numpy array of shape (number of examples, input size)

    Returns:
    X -- normalized data, numpy array of shape (number of examples, input size)
    """
    # Normalization according to rows (axis = 1)
    Y = np.linalg.norm(X, axis=1, keepdims=True)
    X /= Y
    return X


def test_normalize_rows():
    print("\n" + "\033[92m" + "Test normalize_rows() ..." + "\033[0m")
    x = normalize_rows(np.array([[3.0,4.0],[1,2]]))
    print(x)
    ans = np.array([[0.6,0.8],[0.4472136,0.89442719]])
    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)
    print("\033[92m" + "... end test" + "\033[0m")


def initialize_word2vec_parameters(input_vectors, output_vectors):
    """
    Initialize parameters for Word2Vec shallow network

    Arguments:
    input_vectors -- Matrix (m,n) for word coding (embedding)
    output_vectors -- Matrix (m,n) for word decoding

    Returns:
    parameters -- W1, W2, b1, b2 necessary for forward propagation
    hyper_parameters -- For set up the shallow network of Word2Vec
    """
    parameters = {}
    parameters["W1"] = input_vectors.T # Shape (n,m) for word coding (embedding)
    parameters["b1"] = 0
    parameters["W2"] = output_vectors # Shape (m,n) for word decoding
    parameters["b2"] = 0
    hyper_parameters = {}
    hyper_parameters["activations"] = {}
    hyper_parameters["activations"][1] = "linear"
    hyper_parameters["activations"][2] = "softmax"

    return parameters, hyper_parameters


def word2vec_forward_propagation(X, parameters, hyper_parameters):
    """
    Call the forward_propagation on the shallow network

    Arguments:
    X -- One-hot vector representing the central word
    parameters -- W1, W2, b1, b2 necessary for forward propagation
    hyper_parameters -- For set up the shallow network of Word2Vec

    Returns:
    probabilities -- Output of the forward propagation (softmax activation function)
    caches -- Caches generated by the forward propagation for each layer
    """
    probabilities, caches = forward_propagation(X, parameters, hyper_parameters)
    return probabilities, caches


def softmax_gradients(input_vector, output_vectors, probabilities, target_word):
    """ Gradients for one predicted word vector and one target
    word vector as a building block for word2vec models, assuming the
    softmax prediction function and cross entropy loss.

    Arguments:
    input_vector -- matrix (1,n), representation of the center word in the input matrix
    output_vectors -- "output" vectors (as columns) for all tokens (words in the vocabulary)
    probabilities -- output of the forward propagation propagation
    target_word -- one-hot vector representation of the target word

    Return:
    grad_pred -- the gradient with respect to the predicted word vector
    grad -- the gradient with respect to all the other word vectors
    """
    # Backward propagation

    dout = probabilities - target_word # (n_words,1)
    grad_pred = np.dot(dout.T, output_vectors) # (1, dim_embed)
    grad = np.dot(dout, input_vector) # (n_words, dim_embed)
    print("grad")
    print(grad)

    return grad_pred, grad


def skipgram(current_word, C, context_words, tokens, parameters, hyper_parameters,
             dataset, word2vec_gradient=softmax_gradients):
    """ Skip-gram model in Word2Vec

    Arguments:
    current_word -- a string of the current center word
    C -- integer, context size
    context_words -- list of no more than 2*C strings, the context words
    tokens -- a dictionary that maps words to their indices in
              the word vector list
    dataset -- object that defines current_word and context_words
    parameters -- parameters W1, W2, b1, b2 of the shallow network of Word2Vec
    hyper_parameters -- hyper parameters of the shallow network (in this case activation functions)
    word2vec_gradient -- the gradient function for
                               a prediction vector given the target
                               word vectors

    Return:
    cost -- the cost function value for the skip-gram model
    grad -- the gradient with respect to the word vectors
    """
    cost = 0.0
    gradIn = np.zeros(parameters["W1"].shape) # IMPORTANT: Remember that you have transposed this matrix
    gradOut = np.zeros(parameters["W2"].shape)

    idx = tokens[current_word] # tokens['a'] = 0
    X = np.zeros(len(tokens))
    X[idx] = 1 # one-hot vector representing the central word (input of the neural network)
    X = X.reshape(len(tokens), 1) # 1-dimensional matrix

    for context in context_words:
        # Remember that, in Word2Vec, samples are couples of [center word, context word]
        probabilities, caches = word2vec_forward_propagation(X, parameters, hyper_parameters)

        Y = np.zeros(len(tokens))
        Y[tokens[context]] = 1
        Y = Y.reshape(len(tokens),1)

        cost += -np.log(probabilities[tokens[context]]) # TODO: Must be updated with cross entropy function

        input_vector = parameters["W1"][:,idx]
        input_vector = input_vector.reshape(1,parameters["W1"].shape[0])
        output_vectors = parameters["W2"]

        g_in, g_out = word2vec_gradient(input_vector,
                                        output_vectors,
                                        probabilities,
                                        Y)

        gradIn[:,idx] += g_in.reshape(g_in.shape[1])
        gradOut += g_out
        print("gradIn")
        print(gradIn)
        print("gradOut")
        print(gradOut)

    return cost, gradIn, gradOut


#############################################
# Testing functions
#############################################

def word2vec_sgd_wrapper(word2vec_model, tokens, word_vectors, dataset, C,
                         word2vec_gradient=softmax_gradients):
    # It defines number of samples that going to be propagated through the network.
    # It means that each 50 samples you update your parameters (efficient reasons)
    batchsize = 50
    cost = 0.0
    grad = np.zeros(word_vectors.shape) # (m,n) Zero matrix for the gradients
    m = word_vectors.shape[0] # Number of different words in the vocabulary

    # Matrices of parameters for the forward_propagation
    input_vectors = word_vectors[:int(m/2),:]
    output_vectors = word_vectors[int(m/2):,]
    params, h_params = initialize_word2vec_parameters(input_vectors, output_vectors)

    for i in range(batchsize):

        # Randomize center word and context word generation
        C1 = np.random.randint(1,C)
        centerword, context = dataset.getRandomContext(C1) # Example of output: ('c', ['a', 'b', 'e'])

        # Maybe you can remove it
        if word2vec_model == skipgram:
            denom = 1
        else:
            denom = 1

        c, gin, gout = word2vec_model(
            centerword, C1, context, tokens, params, h_params,
            dataset, word2vec_gradient)
        cost += c / batchsize / denom
        grad[:int(m/2),:] += gin.T / batchsize / denom
        grad[int(m/2):,] += gout / batchsize / denom

    return cost, grad


def test_word2vec():
    """
    Test Word2Vec models

    """
    dataset = type('dummy', (), {})() # It creates class dynamically and creates an instance of it

    def dummySampleTokenIdx(): # It generates randomly an int between 0 and 4
        return np.random.randint(0, 4)

    # Example of output: ('b', ['c', 'a'])
    # Example of output: ('c', ['c', 'b', 'e', 'a', 'b', 'e'])
    def getRandomContext(C): # C is equal to the number of elements in the context (window)
        tokens = ["a", "b", "c", "d", "e"]
        return tokens[np.random.randint(0,4)], [tokens[np.random.randint(0,4)] for i in range(2*C)] # C is a window

    dataset.sampleTokenIdx = dummySampleTokenIdx
    dataset.getRandomContext = getRandomContext

    np.random.seed(31415)
    np.random.seed(9265)

    dummy_vectors = normalize_rows(np.random.randn(10,3))
    dummy_tokens = dict([("a",0), ("b",1), ("c",2),("d",3),("e",4)])

    print("\n==== Gradient check for skip-gram ====")
    cost, grad = word2vec_sgd_wrapper(skipgram,
                                      dummy_tokens,
                                      dummy_vectors,
                                      dataset,
                                      5,
                                      softmax_gradients)

if __name__ == "__main__":
    test_normalize_rows()
    test_word2vec()
