#!/usr/bin/env python

import numpy as np
import random
from dnn import *
import sys
sys.path.insert(0, "./activations")
from softmax import *

"""
TODO: Update backpropagation with the implementation in dnn.py
TODO: Extend the use of the forward propagation implementation in dnn.py also for negative sampling
"""

def normalize_rows(X):
    """
    Row normalization function. Implement a function that normalizes each
    row of a matrix to have unit length

    Arguments:
    X -- data, numpy array of shape (number of examples, input size)

    Returns:
    X -- normalized data, numpy array of shape (number of examples, input size)
    """
    # Normalization according to rows (axis = 1)
    Y = np.linalg.norm(X, axis=1, keepdims=True)
    X /= Y
    return X


def initialize_word2vec_parameters(input_vectors, output_vectors):
    """
    Initialize parameters for Word2Vec shallow network

    Arguments:
    input_vectors -- Matrix (m,n) for word coding (embedding)
    output_vectors -- Matrix (m,n) for word decoding

    Returns:
    parameters -- W1, W2, b1, b2 necessary for forward propagation
    hyper_parameters -- For set up the shallow network of Word2Vec
    """
    parameters = {}
    parameters["W1"] = input_vectors.T # Shape (n,m) for word coding (embedding)
    parameters["b1"] = 0
    parameters["W2"] = output_vectors # Shape (m,n) for word decoding
    parameters["b2"] = 0
    hyper_parameters = {}
    hyper_parameters["activations"] = {}
    hyper_parameters["activations"][1] = "linear"
    hyper_parameters["activations"][2] = "softmax"

    return parameters, hyper_parameters


def word2vec_forward_propagation(X, parameters, hyper_parameters):
    """
    Call the forward_propagation on the shallow network

    Arguments:
    X -- One-hot vector representing the central word
    parameters -- W1, W2, b1, b2 necessary for forward propagation
    hyper_parameters -- For set up the shallow network of Word2Vec

    Returns:
    probabilities -- Output of the forward propagation (softmax activation function)
    caches -- Caches generated by the forward propagation for each layer
    """
    probabilities, caches = forward_propagation(X, parameters, hyper_parameters)

    return probabilities, caches


def softmax_cost_grads(input_vector, output_vectors, probabilities, target_word, dataset):
    """ Cost and gradients for one predicted word vector and one target
    word vector as a building block for word2vec models, assuming the
    softmax prediction function and cross entropy loss.

    Arguments:
    input_vector -- matrix (1,n), representation of the center word in the input matrix
    output_vectors -- "output" vectors (as columns) for all tokens (words in the vocabulary)
    probabilities -- output of the forward propagation propagation
    target_word -- one-hot vector representation of the target word
    dataset -- object with sample data useful for the negative sampling (not used here)

    Return:
    cost -- cross-entropy cost
    grad_pred -- the gradient with respect to the predicted word vector
    grad -- the gradient with respect to all the other word vectors
    """
    # Backward propagation
    dout = probabilities - target_word # (n_words,1)
    dout = dout.reshape(probabilities.shape[0]) # (n_words,)
    grad_pred = np.dot(dout, output_vectors) # (1, dim_embed)
    grad = np.dot(dout.reshape(probabilities.shape[0],1), input_vector) # (n_words, dim_embed)

    target_word = target_word.reshape(target_word.shape[0])
    target_word = list(np.where(target_word == 1))[0]
    idx = target_word[0]

    cost = -np.log(probabilities[idx])

    return cost, grad_pred, grad


def negative_sampling(input_vector, output_vectors, probabilities, target_word, dataset, K=10):
    """ Negative sampling cost and gradients function for Word2Vec models

    Arguments:
    input_vector -- matrix (1,n), representation of the center word in the input matrix
    output_vectors -- "output" vectors (as columns) for all tokens (words in the vocabulary)
    probabilities -- output of the forward propagation propagation
    target_word -- one-hot vector representation of the target word
    dataset -- object with sample data useful for the negative sampling


    Returns:
    cost -- cross-entropy cost
    grad_pred -- the gradient with respect to the predicted word vector
    grad -- the gradient with respect to all the other word vectors

    """

    grad_pred = np.zeros_like(input_vector)
    grad = np.zeros_like(output_vectors)

    # target_word is a one-hot matrix, for instance [[0.][0.][0.][1.][0.]]. I need to get [3]
    target_word = target_word.reshape(target_word.shape[0])
    target_word = list(np.where(target_word == 1))[0]
    indices = [target_word[0]]

    # Generate K int numbers for negative sampling
    for k in range(K):
        newidx = dataset.sampleTokenIdx()
        while newidx == target_word:
            newidx = dataset.sampleTokenIdx()
        indices += [newidx]

    directions = np.array([1] + [-1 for k in range(K)])

    N = np.shape(output_vectors)[1]

    output_words = output_vectors[indices,:]
    input_vector = input_vector.reshape(input_vector.shape[1])

    delta, _ = sigmoid(np.dot(output_words, input_vector) * directions)
    delta_minus = (delta - 1) * directions;

    cost = -np.sum(np.log(delta));
    grad_pred = np.dot(delta_minus.reshape(1,K+1), output_words).flatten()
    grad_min = np.dot(delta_minus.reshape(K+1,1), input_vector.reshape(1,N))

    for k in range(K+1):
        grad[indices[k]] += grad_min[k,:]

    return cost, grad_pred, grad


def skipgram(current_word, C, context_words, tokens, parameters, hyper_parameters,
             dataset, word2vec_gradient=softmax_cost_grads):
    """ Skip-gram model in Word2Vec

    Arguments:
    current_word -- a string of the current center word
    C -- integer, context size
    context_words -- list of no more than 2*C strings, the context words
    tokens -- a dictionary that maps words to their indices in
              the word vector list
    dataset -- object that defines current_word and context_words
    parameters -- parameters W1, W2, b1, b2 of the shallow network of Word2Vec
    hyper_parameters -- hyper parameters of the shallow network (in this case activation functions)
    word2vec_gradient -- the gradient function for
                               a prediction vector given the target
                               word vectors

    Return:
    cost -- the cost function value for the skip-gram model
    grad -- the gradient with respect to the word vectors
    """
    cost = 0.0
    grad_in = np.zeros(parameters["W1"].shape) # IMPORTANT: Remember that you have transposed this matrix
    grad_out = np.zeros(parameters["W2"].shape)

    idx = tokens[current_word] # tokens['a'] = 0
    X = np.zeros(len(tokens))
    X[idx] = 1 # one-hot vector representing the central word (input of the neural network)
    X = X.reshape(len(tokens), 1) # 1-dimensional matrix

    for context in context_words:
        # Remember that, in Word2Vec, samples are couples of [center word, context word]
        probabilities, _ = word2vec_forward_propagation(X, parameters, hyper_parameters)

        Y = np.zeros(len(tokens))
        Y[tokens[context]] = 1
        Y = Y.reshape(len(tokens),1)

        input_vector = parameters["W1"][:,idx]
        input_vector = input_vector.reshape(1,parameters["W1"].shape[0])
        output_vectors = parameters["W2"]

        # Cost and Gradients could be caluculated with the softmax or with the negative sampling
        dcost, g_in, g_out = word2vec_gradient(input_vector,
                                        output_vectors,
                                        probabilities,
                                        Y,
                                        dataset)

        cost += dcost
        grad_in[:,idx] += g_in
        grad_out += g_out

    return cost, grad_in, grad_out


def word2vec_sgd_wrapper(word2vec_model, tokens, word_vectors, dataset, C,
                         word2vec_gradient=softmax_cost_grads):
    # It defines number of samples that going to be propagated through the network.
    # It means that each 50 samples you update your parameters (efficient reasons)
    batchsize = 50
    cost = 0.0
    grad = np.zeros(word_vectors.shape) # (m,n) Zero matrix for the gradients
    m = word_vectors.shape[0] # Number of different words in the vocabulary

    # Matrices of parameters for the forward propagation
    input_vectors = word_vectors[:int(m/2),:]
    output_vectors = word_vectors[int(m/2):,]

    params, h_params = initialize_word2vec_parameters(input_vectors, output_vectors)

    for i in range(batchsize):

        # Randomize center word and context word generation
        C1 = random.randint(1,C)
        centerword, context = dataset.getRandomContext(C1) # Example of output: ('c', ['a', 'b', 'e'])

        # Maybe you can remove it
        if word2vec_model == skipgram:
            denom = 1
        else:
            denom = 1

        c, gin, gout = word2vec_model(
            centerword, C1, context, tokens, params, h_params,
            dataset, word2vec_gradient)
        cost += c / batchsize / denom
        grad[:int(m/2),:] += gin.T / batchsize / denom
        grad[int(m/2):,] += gout / batchsize / denom

    return cost, grad


if __name__ == "__main__":
    print("\n"  + "Launch" + "\033[92m" + " python tests/word2vec_test.py " + "\033[0m" + "script to test Word2Vec cost and gradient\n")
